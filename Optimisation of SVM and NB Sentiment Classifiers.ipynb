{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14acad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter pathname to dataset\n",
    "df = pd.read_csv(\"pathname\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f898780",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['target','id','datetime','query','user','tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag column contains only of no query - this column is not value adding\n",
    "df.groupby('query').query.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop irrelevant columns\n",
    "df = df.drop(['datetime','query','user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no missing values present\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff923ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target']==4].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec584d7",
   "metadata": {},
   "source": [
    "## Data Sampling: 1.6 million rows takes too long processing time. Sample 1% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171eb1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweets of negative polarity\n",
    "neg_df = df[df['target'] == 0]\n",
    "\n",
    "#get tweets of positive polarity\n",
    "pos_df = df[df['target'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_subset = neg_df.sample(frac = 0.001, random_state = 55)\n",
    "\n",
    "pos_subset = pos_df.sample(frac = 0.001, random_state = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5741fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.concat([neg_subset, pos_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of original df to retain original data\n",
    "test_df = df_subset.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb261ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891f1cf",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2324ad",
   "metadata": {},
   "source": [
    "#### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply case folding\n",
    "test_df ['clean_tweet'] = test_df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3671a0",
   "metadata": {},
   "source": [
    "#### Removing Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb23458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted elements\n",
    "def remove_ele(input_str, target_element):\n",
    "    r = re.findall(target_element, input_str)\n",
    "    \n",
    "    for i in r:\n",
    "        input_str = re.sub(i, '', input_str)\n",
    "    \n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unwanted elements: @user, punctuations, numbers, hashtags\n",
    "\n",
    "#removing @user\n",
    "test_df['clean_tweet'] = test_df.apply(lambda row: remove_ele(row['clean_tweet'], \"@[\\w]*\"),axis=1)\n",
    "#removing punctuations, numbers, hashtags (non-letter characters)\n",
    "test_df['clean_tweet'] = test_df['clean_tweet'].str.replace(\"[^a-zA-Z\\s_]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ac3a6",
   "metadata": {},
   "source": [
    "#### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def spellingCorrection(input_str):\n",
    "    \n",
    "    textCorrected = spell(input_str)\n",
    "    \n",
    "    return textCorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'] = test_df.apply(lambda row: spellingCorrection(row['clean_tweet']),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad409507",
   "metadata": {},
   "source": [
    "#### Negation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42713b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegationHandling(input_str):\n",
    "    \n",
    "    syntacticNegation = {\"no\",\"not\",\"rather\",\"couldnt\",\"wasnt\",\"didnt\",\"wouldnt\",\"shouldnt\",\n",
    "                   \"werent\",\"dont\",\"doesnt\",\"havent\",\"hasnt\",\"wont\",\"hadnt\",\n",
    "                    \"never\",\"none\",\"nobody\",\"nothing\",\"neither\",\"nor\",\"nowhere\",\"isnt\"\n",
    "                         ,\"cant\",\"cannot\",\"musnt\",\"mightnt\",\"shant\",\"without\",\"neednt\"}\n",
    "    \n",
    "    split_str = input_str.split()\n",
    "    num = 0\n",
    "    limit = len(split_str)-1\n",
    "    syneg = False\n",
    "\n",
    "    while syneg == False and num < limit:\n",
    "        if split_str[num] in syntacticNegation:\n",
    "            split_str[num] += \"_NEG\"\n",
    "            syneg = True\n",
    "        else:\n",
    "            num += 1\n",
    "            syneg = False\n",
    "            continue\n",
    "\n",
    "        while syneg == True and num < limit:\n",
    "            num += 1\n",
    "\n",
    "            if split_str[num].lower() == 'but':\n",
    "                syneg == False\n",
    "                break\n",
    "\n",
    "            elif split_str[num][-1] in string.punctuation:\n",
    "                split_str[num] += \"_NEG\"\n",
    "                syneg == False\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                split_str[num] += \"_NEG\"\n",
    "                continue\n",
    "\n",
    "        if num < limit:\n",
    "            syneg = False\n",
    "\n",
    "    out = \" \".join([x for x in split_str])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement negation handling\n",
    "test_df['clean_tweet'] = test_df.apply(lambda row: NegationHandling(row['clean_tweet']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e8754",
   "metadata": {},
   "source": [
    "#### Duplicated Word Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59439cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicated words\n",
    "def duplicateNormalisation(input_str):\n",
    "\n",
    "    regex_ex = re.compile(r'([^\\W\\d_])\\1{2,}')\n",
    "    tweet = re.sub(r'([^\\W\\d_])\\1{2,}', r'\\1\\1', input_str)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'] = test_df.apply(lambda row: duplicateNormalisation(row['clean_tweet']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tweet'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c30bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490fce8",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46101c8a",
   "metadata": {},
   "source": [
    "### N-Grams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba00dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set.add('')\n",
    "stopwords_set.add(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram=1):\n",
    "    words=[word for word in text.split(\" \") if word not in stopwords_set]  \n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa1303",
   "metadata": {},
   "source": [
    "#### UNIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "positiveValues1=defaultdict(int)\n",
    "negativeValues1=defaultdict(int)\n",
    "\n",
    "for text in test_df[test_df.target==4].tweet:\n",
    "    for word in generate_N_grams(text,1):\n",
    "        positiveValues1[word]+=1\n",
    "\n",
    "for text in test_df[test_df.target==0].tweet:\n",
    "    for word in generate_N_grams(text,1):\n",
    "        negativeValues1[word]+=1\n",
    "\n",
    "df_positive1=pd.DataFrame(sorted(positiveValues1.items(),key=lambda x:x[1],reverse=True))\n",
    "df_negative1=pd.DataFrame(sorted(negativeValues1.items(),key=lambda x:x[1],reverse=True))\n",
    "\n",
    "pd1uni=df_positive1[0][:10]\n",
    "pd2uni=df_positive1[1][:10]\n",
    "\n",
    "ned1uni=df_negative1[0][:10]\n",
    "ned2uni=df_negative1[1][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(pd1uni,pd2uni, color ='green',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Words in positive dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in positive dataframe-UNIGRAM ANALYSIS\")\n",
    "plt.savefig('unigram_pos.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(ned1uni,ned2uni, color ='orange',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Words in negative dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in negative dataframe-UNIGRAM ANALYSIS\")\n",
    "plt.savefig('unigram_neg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e9e18",
   "metadata": {},
   "source": [
    "#### BIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "positiveValues2=defaultdict(int)\n",
    "negativeValues2=defaultdict(int)\n",
    "\n",
    "for text in test_df[test_df.target==4].tweet:\n",
    "    for word in generate_N_grams(text,2):\n",
    "        positiveValues2[word]+=1\n",
    "\n",
    "for text in test_df[test_df.target==0].tweet:\n",
    "    for word in generate_N_grams(text,2):\n",
    "        negativeValues2[word]+=1\n",
    "\n",
    "df_positive2=pd.DataFrame(sorted(positiveValues2.items(),key=lambda x:x[1],reverse=True))\n",
    "df_negative2=pd.DataFrame(sorted(negativeValues2.items(),key=lambda x:x[1],reverse=True))\n",
    "\n",
    "pd1bi=df_positive2[0][:10]\n",
    "pd2bi=df_positive2[1][:10]\n",
    "\n",
    "ned1bi=df_negative2[0][:10]\n",
    "ned2bi=df_negative2[1][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(pd1bi,pd2bi, color ='green',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Words in positive dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in positive dataframe-BIGRAM ANALYSIS\")\n",
    "plt.savefig('bigram_pos.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(ned1bi,ned2bi, color ='orange',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Words in negative dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in negative dataframe-BIGRAM ANALYSIS\")\n",
    "plt.savefig('bigram_neg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7f566",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "positiveValues3=defaultdict(int)\n",
    "negativeValues3=defaultdict(int)\n",
    "\n",
    "for text in test_df[test_df.target==4].tweet:\n",
    "    for word in generate_N_grams(text,3):\n",
    "        positiveValues3[word]+=1\n",
    "\n",
    "for text in test_df[test_df.target==0].tweet:\n",
    "    for word in generate_N_grams(text,3):\n",
    "        negativeValues3[word]+=1\n",
    "        \n",
    "df_positive3=pd.DataFrame(sorted(positiveValues3.items(),key=lambda x:x[1],reverse=True))\n",
    "df_negative3=pd.DataFrame(sorted(negativeValues3.items(),key=lambda x:x[1],reverse=True))\n",
    "\n",
    "\n",
    "pd1tri=df_positive3[0][0:10]\n",
    "pd2tri=df_positive3[1][0:10]\n",
    "\n",
    "ned1tri=df_negative3[0][0:10]\n",
    "ned2tri=df_negative3[1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(30,4))\n",
    "plt.bar(pd1tri,pd2tri, color ='green',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Positive Trigram Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in positive dataframe-TRIGRAM ANALYSIS\")\n",
    "plt.savefig('trigram_pos.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(ned1tri,ned2tri, color ='orange',\n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Negative Trigram Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in negative dataframe-TRIGRAM ANALYSIS\")\n",
    "plt.savefig('trigram_neg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e82976",
   "metadata": {},
   "source": [
    "## Data Partition (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e35648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df['clean_tweet'], test_df['target'], test_size = 0.2, random_state = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f0c36",
   "metadata": {},
   "source": [
    "### N-Grams Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52426b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,1)).fit(X_train)\n",
    "X_train_ngrams_vect_uni = ngrams_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475da808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni-bi-trigram vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,3)).fit(X_train)\n",
    "X_train_ngrams_vect_unitri = ngrams_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b152cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ngrams_vect.get_feature_names_out()) #unigrams - 20455, #bigrams - 85908, #trigrams - 114527, #uni, bi, trigrams - 220890"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7318ecf",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(min_df = 1).fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d27ea",
   "metadata": {},
   "source": [
    "## Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa288c9",
   "metadata": {},
   "source": [
    "### Model A: SVM with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "modelA = SVC()\n",
    "\n",
    "# fit on n-grams vectorised\n",
    "modelA.fit(X_train_ngrams_vect_uni, y_train)\n",
    "\n",
    "predictionsA = modelA.predict(ngrams_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5132648",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA._gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictionsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictionsA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5838701",
   "metadata": {},
   "source": [
    "### Model B: SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "modelB = SVC()\n",
    "\n",
    "modelB.fit(X_train_tfidf_vect, y_train)\n",
    "\n",
    "predictionsB = modelB.predict(tfidf_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictionsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cba8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsB)) #75.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3c606",
   "metadata": {},
   "source": [
    "### Model C: Naive Bayes with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "modelC = MultinomialNB()\n",
    "\n",
    "modelC.fit(X_train_ngrams_vect_uni,y_train)\n",
    "\n",
    "predictionsC = modelC.predict(ngrams_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictionsC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsC)) #73.89%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959b8d8",
   "metadata": {},
   "source": [
    "### Model D: Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "modelD = MultinomialNB()\n",
    "\n",
    "modelD.fit(X_train_tfidf_vect,y_train)\n",
    "predictionsD = modelD.predict(tfidf_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictionsD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c162d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsD)) #73.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b86635",
   "metadata": {},
   "source": [
    "### Model E and F: N-Grams + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df=1).fit(X_train)\n",
    "X_train_tfidf_vect_E = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd463134",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm with n-grams and tfidf\n",
    "modelE = SVC()\n",
    "modelE.fit(X_train_tfidf_vect_3a, y_train)\n",
    "predictionsE = modelE.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c608e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,3), min_df=1).fit(X_train)\n",
    "X_train_tfidf_vect_F = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb with n-grams and tfidf\n",
    "modelF = MultinomialNB()\n",
    "modelF.fit(X_train_tfidf_vect_2f,y_train)\n",
    "predictionsF = model.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsD))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e3f72",
   "metadata": {},
   "source": [
    "## Optimising with Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51300c8f",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "#### Feature Selection using Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate information gain value for terms in X_train_ngrams_vect\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "res_ngrams = dict(zip(ngrams_vect.get_feature_names_out(),\n",
    "              mutual_info_classif(X_train_ngrams_vect_uni,y_train,discrete_features=True)))\n",
    "\n",
    "print(res_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d0a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate information gain value for terms in X_train_tfidf_vect\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "res_tfidf = dict(zip(tfidf_vect.get_feature_names_out(),\n",
    "              mutual_info_classif(X_train_tfidf_vect,y_train,discrete_features=True)))\n",
    "\n",
    "print(res_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate information gain value for terms in X_train_tfidf_vect\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "res_tfidf = dict(zip(tfidf_vect.get_feature_names_out(),\n",
    "              mutual_info_classif(X_train_tfidf_vect_E,y_train,discrete_features=True)))\n",
    "\n",
    "print(res_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_items = res_ngrams.items()\n",
    "ngrams_list = list(ngrams_items)\n",
    "\n",
    "df_ngrams = pd.DataFrame(ngrams_list)\n",
    "df_ngrams = df_ngrams.sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_items = res_tfidf.items()\n",
    "tfidf_list = list(tfidf_items)\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_list)\n",
    "df_tfidf = df_tfidf.sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bf3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get threshold value to retain top 20%\n",
    "def ig_threshold(vectoriser):\n",
    "    if vectoriser == 'n-grams':\n",
    "        df = df_ngrams\n",
    "    else:\n",
    "        df = df_tfidf\n",
    "        \n",
    "    top20_count = 0.2*(df.count())\n",
    "    top20_count = int(top20_count[0])\n",
    "    top20_threshold = df[1].iloc[top20_count]\n",
    "    return(top20_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words that are have information gain lower than top20_threshold\n",
    "def ig_selection(input_str,top20_threshold, res):\n",
    "    \n",
    "    for word in input_str.split(\" \"):\n",
    "        ig_val = res.get(word)\n",
    "        if ig_val:\n",
    "            if res[word] < top20_threshold:\n",
    "                input_str = input_str.replace(word, '')\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ig_threshold('n-grams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691aae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ig_threshold('tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter according to ig_ngrams\n",
    "top20_threshold = ig_threshold('n-grams')\n",
    "\n",
    "test_df['ngrams_ig_tweet'] = test_df.apply(lambda row: \n",
    "                                               ig_selection(row['clean_tweet'],top20_threshold, res_ngrams),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter according to ig_tfidf\n",
    "top20_threshold = ig_threshold('tf-idf')\n",
    "\n",
    "test_df['tfidf_ig_tweet'] = test_df.apply(lambda row: \n",
    "                                                ig_selection(row['clean_tweet'],top20_threshold, res_tfidf),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat vectorisation again with n-grams and tf-idf and apply SVM and NB\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df['ngrams_ig_tweet'], test_df['target'], test_size = 0.2, random_state = 50)\n",
    "\n",
    "#tfidf\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "\n",
    "#uni-grams\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,1)).fit(X_train)\n",
    "X_train_ngrams_vect = ngrams_vect.transform(X_train)\n",
    "\n",
    "#svm with n-grams\n",
    "modelF = SVC()\n",
    "modelF.fit(X_train_ngrams_vect, y_train)\n",
    "predictionsF = modelF.predict(ngrams_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsF))\n",
    "\n",
    "#svm with tfidf\n",
    "modelG = SVC()\n",
    "modelG.fit(X_train_tfidf_vect, y_train)\n",
    "predictionsG = modelG.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsG))\n",
    "\n",
    "#uni-tri-grams\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,3)).fit(X_train)\n",
    "X_train_ngrams_vect = ngrams_vect.transform(X_train)\n",
    "\n",
    "\n",
    "#nb with n-grams\n",
    "modelH = MultinomialNB()\n",
    "modelH.fit(X_train_ngrams_vect,y_train)\n",
    "predictionsH = modelH.predict(ngrams_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsH))\n",
    "\n",
    "#nb with tfidf\n",
    "modelI = MultinomialNB()\n",
    "modelI.fit(X_train_tfidf_vect,y_train)\n",
    "predictionsI = modelI.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86203e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsF))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsG))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsH))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM/NB + ngrams + tfidf + information gain\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df['tfidf_ig_tweet'], test_df['target'], test_size = 0.2, random_state = 50)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=1).fit(X_train)\n",
    "X_train_tfidf_vect_F = tfidf_vect.transform(X_train)\n",
    "\n",
    "#svm with tfidf\n",
    "modelL = SVC()\n",
    "modelL.fit(X_train_tfidf_vect_F, y_train)\n",
    "predictionsL = modelL.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsL))\n",
    "\n",
    "#nb with tfidf\n",
    "modelM = MultinomialNB()\n",
    "modelM.fit(X_train_tfidf_vect_F,y_train)\n",
    "predictionsM = modelM.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsM))\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsL))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee735371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat vectorisation again with n-grams and tf-idf and apply SVM and NB\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df['tfidf_ig_tweet'], test_df['target'], test_size = 0.2, random_state = 50)\n",
    "\n",
    "#n-grams\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,2)).fit(X_train)\n",
    "X_train_ngrams_vect = ngrams_vect.transform(X_train)\n",
    "\n",
    "#tfidf\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "\n",
    "#svm with n-grams\n",
    "modelE = SVC()\n",
    "modelE.fit(X_train_ngrams_vect, y_train)\n",
    "predictionsE = modelE.predict(ngrams_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsE))\n",
    "\n",
    "#svm with tfidf\n",
    "modelF = SVC()\n",
    "modelF.fit(X_train_tfidf_vect, y_train)\n",
    "predictionsF = modelF.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsF))\n",
    "\n",
    "#nb with n-grams\n",
    "modelG = MultinomialNB()\n",
    "modelG.fit(X_train_ngrams_vect,y_train)\n",
    "predictionsG = modelG.predict(ngrams_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsG))\n",
    "\n",
    "#nb with tfidf\n",
    "modelH = MultinomialNB()\n",
    "modelH.fit(X_train_tfidf_vect,y_train)\n",
    "predictionsH = modelH.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictionsH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b269f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, predictionsE))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsF))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsG))\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictionsH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4006e01",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb1eac",
   "metadata": {},
   "source": [
    "SVM + N-Grams + HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(test_df['clean_tweet'], test_df['target'], test_size = 0.2, random_state = 50)\n",
    "\n",
    "#n-grams\n",
    "ngrams_vect = CountVectorizer(ngram_range = (1,1)).fit(X_train)\n",
    "X_train_ngrams_vect = ngrams_vect.transform(X_train)\n",
    "\n",
    "#tfidf\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ba8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C':[0.1, 1, 10, 100, 1000],\n",
    "             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "             'kernel':['rbf']}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "\n",
    "grid.fit(X_train_ngrams_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(ngrams_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ca7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba6b40",
   "metadata": {},
   "source": [
    "SVM + TF-IDF + HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cd73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C':[0.1, 1, 10, 100, 1000],\n",
    "             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "             'kernel':['rbf']}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "\n",
    "grid.fit(X_train_tfidf_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6750168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(tfidf_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430c325",
   "metadata": {},
   "source": [
    "#### Optimisation by combining NGrams+TFIDF and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0611c4",
   "metadata": {},
   "source": [
    "Perform hyperparameter tuning for SVM + N-Grams + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C':[0.1, 1, 10, 100, 1000],\n",
    "             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "             'kernel':['rbf']}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "\n",
    "grid.fit(X_train_tfidf_vect_E, y_train)\n",
    "\n",
    "grid_predictions = grid.predict(tfidf_vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abefa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cab7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, grid_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
